{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b00d43",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "\n",
    "1. get new datasets\n",
    "    - UCI (NodeFlow and kiran paper) both has target scaling of [-1,1] for all input features and target\n",
    "2. implement kiran mlp resnet\n",
    "3. run it on datassts from papaers (nodeflow) or any paper that is good \n",
    "4. check if results match with the nodeflow paper , or openML-ctr-23\n",
    "\n",
    "- torch nn.embeddings for cateogical data later on.\n",
    "\n",
    "- look at kiran resnet/mlp for univariate regression then:\n",
    "\n",
    "Obseevations:\n",
    "- tabpfn uses ctr23 and AutoML, what a suprrise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6e8c3",
   "metadata": {},
   "source": [
    "# Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c491c568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']\n",
      "[False, False, True, True, False, False, False, False, False, False, False, False]\n",
      "['month', 'day']\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "\n",
    "\n",
    "dataset = openml.datasets.get_dataset(44962)\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)  \n",
    "\n",
    "# Manual fix for forest_fires dataset (ID 44962) with day and month\n",
    "if True:\n",
    "    categorical_indicator[2] = True  # month\n",
    "    categorical_indicator[3] = True  # day\n",
    "\n",
    "\n",
    "categorical_cols = [col for col, is_cat in zip(X.columns, categorical_indicator) if is_cat]\n",
    "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "print(attribute_names)\n",
    "print(categorical_indicator)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01d2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:36:00 - INFO: fetching auction_verification (44958) from openML.\n",
      "17:36:00 - INFO: One-hot encoding categorical features for dataset auction_verification: ['property.product', 'property.winner']\n",
      "17:36:00 - INFO: fetching airfoil_self_noise (44957) from openML.\n",
      "17:36:00 - INFO: fetching brazilian_houses (44990) from openML.\n",
      "17:36:00 - INFO: One-hot encoding categorical features for dataset brazilian_houses: ['city', 'floor', 'animal', 'furniture']\n",
      "17:36:00 - INFO: fetching cars (44994) from openML.\n",
      "17:36:00 - INFO: fetching forest_fires (44962) from openML.\n",
      "17:36:00 - INFO: fetching QSAR_fish_toxicity (44970) from openML.\n",
      "17:36:01 - INFO: fetching space_ga (45402) from openML.\n",
      "17:36:01 - INFO: fetching socmob (44987) from openML.\n",
      "17:36:01 - INFO: One-hot encoding categorical features for dataset socmob: ['fathers_occupation', 'sons_occupation', 'family_structure', 'race']\n",
      "17:36:01 - INFO: fetching geographical_origin_of_music (44965) from openML.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical columns in auction_verification: ['process.b1.capacity', 'process.b2.capacity', 'process.b3.capacity', 'process.b4.capacity', 'property.price']\n",
      "Categorical columns in auction_verification: []\n",
      "\n",
      "numerical columns in airfoil_self_noise: ['frequency', 'angle_of_attack', 'chord_length', 'free_stream_velocity', 'displacement_thickness']\n",
      "Categorical columns in airfoil_self_noise: []\n",
      "\n",
      "numerical columns in brazilian_houses: ['area', 'rooms', 'bathroom', 'parking_spaces', 'hoa']\n",
      "Categorical columns in brazilian_houses: []\n",
      "\n",
      "numerical columns in cars: ['Mileage', 'Cylinder', 'Doors', 'Cruise', 'Sound', 'Leather', 'Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab', 'Saturn', 'convertible', 'coupe', 'hatchback', 'sedan', 'wagon']\n",
      "Categorical columns in cars: []\n",
      "\n",
      "numerical columns in forest_fires: ['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']\n",
      "Categorical columns in forest_fires: ['month', 'day']\n",
      "\n",
      "numerical columns in QSAR_fish_toxicity: ['CIC0', 'SM1_Dz', 'GATS1i', 'NdsCH', 'NdssC', 'MLOGP']\n",
      "Categorical columns in QSAR_fish_toxicity: []\n",
      "\n",
      "numerical columns in space_ga: ['pop', 'education', 'houses', 'income', 'xcoord', 'ycoord']\n",
      "Categorical columns in space_ga: []\n",
      "\n",
      "numerical columns in socmob: ['counts_for_sons_first_occupation']\n",
      "Categorical columns in socmob: []\n",
      "\n",
      "numerical columns in geographical_origin_of_music: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116']\n",
      "Categorical columns in geographical_origin_of_music: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.config import DATASETS\n",
    "from utils.data_loader import load_preprocessed_data\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. get new datasets\n",
    "    - UCI (NodeFlow and kiran paper) both has target scaling of [-1,1] for all input features and target\n",
    "    - multivariate ones are downloaded from https://github.com/gpapamak/maf/tree/master?tab=readme-ov-file#how-to-get-the-datasets\n",
    "      they are already in procssed format and pre-processing is taken also from there\n",
    "        - hepmass has no regression, mode details in  todo, for now adapted naval as multi\n",
    "        - miniboone also has no regression, in todo\n",
    "    - OpenML all done, auto encoder as per the paper they used.\n",
    "\n",
    "2. implement kiran mlp resnet\n",
    "\n",
    "3. run it on datassts from papaers (nodeflow) or any paper that is good \n",
    "4. check if results match with the nodeflow paper , or openML-ctr-23\n",
    "\n",
    "- torch nn.embeddings for cateogical data later on.\n",
    "\n",
    "- look at kiran resnet/mlp for univariate regression then:\n",
    "\n",
    "Obseevations:\n",
    "- tabpfn uses ctr23 and AutoML, what a suprrise!\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    source = \"openml_ctr23\"\n",
    "    print_info = False\n",
    "\n",
    "    for dataset_key, dataset_info in DATASETS[source].items():\n",
    "        X_train, X_test, X_val, y_train, y_test, y_val, dataset_name = load_preprocessed_data(source, dataset_key)\n",
    "\n",
    "        # Identify categorical or non-numeric columns\n",
    "        numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Infer categorical columns (object or category dtypes)\n",
    "        # This assumes that if a column is not numeric, it's categorical.\n",
    "        # Or, if you have explicit categorical_indicator from openml, use that.\n",
    "        potential_cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        if print_info:\n",
    "            print(f\"Testing dataset: {dataset_key} ({dataset_info['name']})\")\n",
    "            print(f\"  X_train shape: {X_train.shape}\")\n",
    "            print(f\"  X_test shape: {X_test.shape}\")\n",
    "            print(f\"  X_val shape: {X_test.shape}\")\n",
    "            print(f\"  y_train shape: {y_train.shape}\")\n",
    "            print(f\"  y_test shape: {y_test.shape}\")\n",
    "            print(f\"  y_val shape: {y_val.shape}\\n\")\n",
    "\n",
    "        print(f\"numerical columns in {dataset_name}: {numerical_cols}\")\n",
    "        print(f\"Categorical columns in {dataset_name}: {potential_cat_cols}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846bc465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:40 - INFO: fetching Concrete Compressive Strength (concrete) from UCI.\n",
      "20:43:41 - INFO: Processing Concrete Compressive Strength...\n",
      "20:43:41 - INFO: fetching Energy Efficiency (energy) from UCI.\n",
      "20:43:41 - INFO: Processing Energy Efficiency...\n",
      "20:43:41 - INFO: fetching Condition Based Maintenance of Naval Propulsion Plants (naval) from uci as zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concrete dataset, N=1030, d=8\n",
      "Dataset: Concrete Compressive Strength\n",
      "Number of numerical features: 10\n",
      "Train loader batches: 1, Val loader batches: 1, Test loader batches: 1\n",
      "energy dataset, N=768, d=8\n",
      "Dataset: Energy Efficiency\n",
      "Number of numerical features: 10\n",
      "Train loader batches: 1, Val loader batches: 1, Test loader batches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:41 - INFO: Processing Condition Based Maintenance of Naval Propulsion Plants...\n",
      "20:43:41 - INFO: fetching Combined Cycle Power Plant (power) from ucirepo.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Condition Based Maintenance of Naval Propulsion Plants\n",
      "Number of numerical features: 17\n",
      "Train loader batches: 5, Val loader batches: 2, Test loader batches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:44 - INFO: Processing Combined Cycle Power Plant...\n",
      "20:43:44 - INFO: fetching Physicochemical Properties of Protein Tertiary Structure (protein) from UCI.\n",
      "20:43:45 - INFO: Processing Physicochemical Properties of Protein Tertiary Structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Combined Cycle Power Plant\n",
      "Number of numerical features: 4\n",
      "Train loader batches: 4, Val loader batches: 1, Test loader batches: 1\n",
      "protein dataset, N=45730, d=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:45 - INFO: fetching Wine Quality (wine) from UCI.\n",
      "20:43:45 - INFO: Processing Wine Quality...\n",
      "20:43:45 - INFO: fetching Yacht Hydrodynamics (yacht) from UCI.\n",
      "20:43:45 - INFO: Processing Yacht Hydrodynamics...\n",
      "20:43:45 - INFO: fetching Kinematics 8nm, ID: 189 from OpenML.\n",
      "20:43:45 - INFO: Processing Kinematics 8nm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Physicochemical Properties of Protein Tertiary Structure\n",
      "Number of numerical features: 11\n",
      "Train loader batches: 17, Val loader batches: 5, Test loader batches: 3\n",
      "wine dataset, N=1599, d=11\n",
      "Dataset: Wine Quality\n",
      "Number of numerical features: 13\n",
      "Train loader batches: 1, Val loader batches: 1, Test loader batches: 1\n",
      "yacht dataset, N=308, d=6\n",
      "Dataset: Yacht Hydrodynamics\n",
      "Number of numerical features: 8\n",
      "Train loader batches: 1, Val loader batches: 1, Test loader batches: 1\n",
      "Dataset: Kinematics 8nm\n",
      "Number of numerical features: 8\n",
      "Train loader batches: 3, Val loader batches: 1, Test loader batches: 1\n"
     ]
    }
   ],
   "source": [
    "from configs.config import DATASETS\n",
    "from utils.data_loader import load_preprocessed_data\n",
    "\n",
    "source = \"uci\"\n",
    "# For looping through all datasets in the source\n",
    "datasets_to_run = DATASETS.get(source, {})\n",
    "\n",
    "\n",
    "for dataset_key, dataset_info_dict in datasets_to_run.items():\n",
    "    dataset_display_name = dataset_info_dict.get('name', dataset_key)\n",
    "    #print(f\"\\n--- Processing dataset: {dataset_key} ({dataset_display_name}) ---\")\n",
    "\n",
    "    train_loader, val_loader, test_loader, num_numerical_features, dataset_name = \\\n",
    "                load_preprocessed_data(source, dataset_key, batch_size=2048)\n",
    "    \n",
    "    if True:\n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        print(f\"Number of numerical features: {num_numerical_features}\")\n",
    "        print(f\"Train loader batches: {len(train_loader)}, Val loader batches: {len(val_loader)}, Test loader batches: {len(test_loader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
